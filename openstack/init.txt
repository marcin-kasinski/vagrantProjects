
    cd /vagrantprojects/openstack/
    
    
    #vagrant plugin install vagrant-disksize
    #vagrant plugin install vagrant-persistent-storage
    
	vagrant halt
	vagrant box update
	
	

	vboxmanage unregistervm openstackmaster --delete
	vboxmanage unregistervm node1 --delete

	vboxmanage list vms

    vagrant up openstackmaster --provision 
    
    

	
	
	vagrant ssh openstackmaster 


	
	#tylko node1
	  cd /vagrantprojects/openstack/
	vagrant halt node1
	
	vboxmanage list vms
	
		vboxmanage unregistervm node1 --delete
	
	 vagrant up node1 --provision
	
	vagrant ssh node1
	
	
	
	#tylko node2
	  cd /vagrantprojects/openstack/
	vagrant halt node2
	
	vboxmanage list vms
	
		vboxmanage unregistervm node2 --delete
	
	 vagrant up node2 --provision
	
	vagrant ssh node2
	
		
	
	
	
	
	#discover 
	openstack compute service list --service nova-compute
	
	su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova
	
	
	
	
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         mkrouter.local  0.0.0.0         UG    0      0        0 enp0s8
10.0.2.0        *               255.255.255.0   U     0      0        0 enp0s3
192.168.1.0     *               255.255.255.0   U     0      0        0 enp0s8
	
	
	change default route
	ip route change default via 192.168.1.11 dev enp0s8
	
	
	    ip netns
    qrouter-ff515318-54a0-46d6-99bf-966e70b87c64
    qdhcp-a924fe7c-90ff-46f3-8b40-ffb482a17284

    vagrant@openstackmaster:~$ sudo ip netns exec qrouter-ff515318-54a0-46d6-99bf-966e70b87c64 ip address


----------------------------------------------------------------------------------------------------------------------------
adding disk
	VBoxManage showvminfo openstackmaster


----------------------------------------------------------------------------------------------------------------------------

on controller

devstack/unstack.sh
devstack/stack.sh

on node1

sudo systemctl restart devstack@n-cpu.service
sudo systemctl status devstack@n-cpu.service
sudo journalctl -f --unit  devstack@n-cpu.service

on controller

	sudo nova-manage cell_v2 discover_hosts --verbose 

      source devstack/openrc admin admin

      openstack security group create SSH
	  openstack security group rule create --proto tcp --dst-port 22 SSH

      openstack security group create ICMP	  
	  openstack security group rule create --proto icmp --dst-port 0 ICMP
      
	
chmod u+x /vagrant/scripts/automate.sh

/vagrant/scripts/automate.sh


clear gateway router1
	

openstack port list


openstack port show  fb026600-4ace-4187-b953-a3bc46ed1552

openstack subnet list 

openstack subnet delete public-subnet

openstack subnet delete ipv6-public-subnet

openstack subnet list 

openstack network list

openstack network delete public

openstack network list

sudo ovs-vsctl list-br

sudo ovs-vsctl list-ports br-ex 


sudo ovs-vsctl add-port br-ex eth2 # sudo ovs-vsctl add-port br-ex enp0s9


sudo ovs-vsctl list-ports br-ex 



openstack network create --provider-physical-network public --provider-network-type flat --external public  


openstack subnet create --subnet-range 192.168.1.0/24 --no-dhcp --gateway 192.168.1.1 --network public --allocation-pool start=192.168.1.20,end=192.168.1.25 public-subnet




openstack router set --external-gateway public myrouter 


sudo ip link show eth2 

sudo ip link set dev eth2 up 

sudo ip link show eth2 


sudo ip netns ls


sudo ip netns exec qrouter-93f15211-30ba-44dc-8e0e-4cecafe4c3fd ping 192.168.1.1



----------------------------------------------------------------------------------------------------------------------------

CINDER
https://www.digitalocean.com/community/tutorials/how-to-use-lvm-to-manage-storage-devices-on-ubuntu-16-04

listuje dyski

lsblk -o NAME,FSTYPE,SIZE,MOUNTPOINT,LABEL
sudo lvmdiskscan
sudo  lvmdiskscan -l
sudo pvscan
sudo pvs
sudo pvdisplay
sudo pvdisplay -m

#volume group
sudo vgscan
sudo vgs -o +devices,lv_path
sudo vgdisplay -v

sudo lvscan
sudo lvs
sudo lvs --segments
sudo lvdisplay -m

remove logical volume
umount /dev/MKmyvolgroup/vps
lvremove /dev/MKmyvolgroup/vps

#Creating Physical Volumes From Raw Storage Devices
sudo lvmdiskscan


sudo pvcreate /dev/sdc

#Creating a New Volume Group from Physical Volumes

sudo vgcreate MK_volume_group_name /dev/sdc

#Creating a Logical Volume by Specifying Size

For instance, to create a 10G logical volume named test from the LVMVolGroup volume group, type:

sudo lvcreate -L 10G -n MK_logical_volume_test MK_volume_group_name

#Creating a Logical Volume From All Remaining Free Space

sudo lvcreate -l 100%FREE -n MK_logical_volume_test MK_volume_group_name


Wyedytowac /etc/cinder/cinder.conf

enabled_backends = lvmdriver-1,lvmdriver-2XXX



[lvmdriver-2]
image_volume_cache_enabled = True
volume_clear = zero
lvm_type = default
iscsi_helper = tgtadm
volume_group = MKmyvolgroup
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver
volume_backend_name = lvmdriver-2


Listujemy serwisy 


systemctl list-units devstack@*


Restart Cinder Services.

sudo systemctl restart devstack@c-api.service
sudo systemctl restart devstack@c-vol.service
sudo systemctl restart devstack@c-sch.service


sudo systemctl restart devstack@c-api.service
sudo journalctl -f --unit devstack@c-api.service

sudo systemctl restart devstack@c-vol.service
sudo journalctl -f --unit devstack@c-vol.service

sudo systemctl restart devstack@c-sch.service
sudo journalctl -f --unit devstack@c-sch.service

sudo systemctl restart devstack@n-cpu.service
sudo journalctl -f --unit devstack@n-cpu.service


sudo journalctl --unit devstack@c-vol.service | grep MKmyvolgroup

#win2linux
sudo sed -i -e 's/\r//g'  /etc/cinder/cinder.conf



cd devstack && source openrc admin admin



 openstack volume type create --public LVM2
 
openstack volume type set LVM2 --property volume_backend_name=lvmdriver-2 


cinder get-pools


openstack volume create --size 20 20gb-vol_LVM2 --type LVM2
openstack volume create --size 40 40gb-vol_LVM2 --type LVM2

openstack volume list


###########loop

openstack volume create --size 4 4gb-vol --type lvmdriver-1 
openstack volume list



----------------------------------------------------------------------------------------------------------------------------

montowanie dysku


listowanie dysków
lsblk -io KNAME,TYPE,SIZE,MODEL

listowanie partycji
sudo fdisk -l 

tworzenie partycji na dysku


fdisk /dev/vdc


The basic fdisk commands you need are:

    m – print help
    p – print the partition table
    n – create a new partition
    d – delete a partition
    q – quit without saving changes
    w – write the new partition table and exit
    
    Format 
    
sudo mkfs.ext3 /dev/vdc1

Mount

sudo mkdir /disk1
sudo mount /dev/vdc1 /disk1
sudo df -H

UMount

sudo umount /dev/vdc1


install ubuntu image

wget -O /tmp/xenial-server-cloudimg-amd64-disk1.img http://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img



source devstack/openrc admin admin


openstack image create --disk-format qcow2  --container-format bare --public --file /tmp/xenial-server-cloudimg-amd64-disk1.img ubuntu-image


boot vm

openstack server create --image ubuntu-image --flavor m1.small --key-name mykey --network mynetwork1 --security-group SSHandICMP --availability-zone nova myvm_ubuntu
                         
                         
                         


logowanie do aszyny za pomoca klucza

ssh -i ~/.ssh/mykey  ubuntu@192.168.1.99


ssh -i /vagrantprojects/private_key ubuntu@192.168.1.99




systemctl list-units devstack@*
systemctl list-units | grep rab*

list failed services


journalctl -b0 _PID=1 | grep Failed




sudo systemctl restart rabbitmq-server
sudo systemctl status rabbitmq-server
sudo journalctl -f --unit  rabbitmq-server



sudo systemctl restart devstack@c-sch
sudo systemctl status devstack@c-sch
sudo journalctl -f --unit devstack@c-sch

sudo systemctl restart devstack@n-cond
sudo systemctl status devstack@n-cond
sudo journalctl -f --unit devstack@n-cond




sudo systemctl restart devstack@c-sch
sudo systemctl status devstack@c-sch
sudo journalctl -f --unit devstack@c-sch


sudo systemctl restart devstack@n-cpu
sudo systemctl status devstack@n-cpu
sudo journalctl -f --unit devstack@n-cpu
